{"id":"INC-023","title":"Kubernetes DNS resolution intermittent failure","summary":"Multiple application pods across namespaces reported intermittent DNS resolution failures. Service-to-service calls failed with 'Temporary failure in name resolution' approximately 15% of the time. CoreDNS pods in kube-system were crash-looping with OOMKilled status. Investigation revealed that application containers were generating excessive DNS queries due to the default ndots:5 configuration in resolv.conf, causing each external hostname lookup to attempt 5 search domain suffixes before querying the actual FQDN. CoreDNS memory usage spiked to 512Mi under load.","source":"k8s-docs","root_cause":"Default Kubernetes resolv.conf ndots:5 setting caused every external DNS query (e.g., api.stripe.com) to first try api.stripe.com.default.svc.cluster.local, api.stripe.com.svc.cluster.local, api.stripe.com.cluster.local, api.stripe.com.us-east-1.compute.internal, and api.stripe.com.ec2.internal before resolving the actual hostname. This 5x query amplification overwhelmed CoreDNS pods which had a 256Mi memory limit. CoreDNS cache was too small to absorb the query volume.","resolution":"Applied pod-level dnsConfig with ndots:2 and added trailing dots to external hostnames in application configuration. Increased CoreDNS memory limit to 1Gi and enabled cache plugin with 300-second TTL and 9984 max entries. Deployed NodeLocal DNSCache as a DaemonSet to reduce cross-node DNS traffic. DNS failure rate dropped from 15% to 0.01%.","services":["coredns","kube-dns","application-pods","nodelocal-dnscache"],"service":"coredns","severity":"P2","timestamp":"2026-01-15T18:45:00Z","tags":["kubernetes","dns","coredns","ndots","resolv-conf","oomkilled","dnscache"]}