{"id":"INC-026","title":"NVIDIA driver mismatch after kernel update causing CUDA failure","summary":"GPU worker nodes in the training cluster stopped running CUDA workloads after an automatic kernel update from 5.15.0-91 to 5.15.0-94. All pods requesting nvidia.com/gpu resources failed with 'CUDA initialization error: CUDA driver version is insufficient for CUDA runtime version'. nvidia-smi returned 'NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver'. The NVIDIA driver module 535.129.03 was compiled against kernel headers for 5.15.0-91 and could not load on the new kernel. Affected 8 GPU nodes running A100 80GB GPUs, halting all distributed training jobs for 3 hours.","source":"github-issues","root_cause":"Unattended-upgrades on Ubuntu 22.04 automatically installed a kernel update which invalidated the pre-compiled NVIDIA driver kernel module. DKMS was installed but the nvidia-dkms-535 package was not configured, so the driver module was not automatically rebuilt for the new kernel. The node was rebooted by kured (Kubernetes Reboot Daemon) after detecting the pending reboot flag, which loaded the new kernel without a working NVIDIA driver.","resolution":"Installed nvidia-dkms-535 package to enable automatic driver module rebuilding via DKMS on kernel updates. Rebuilt the NVIDIA driver module for kernel 5.15.0-94 with dkms install nvidia/535.129.03. Configured kured to exclude GPU nodes from automatic reboots by adding a node annotation kured.dev/reboot-blocked=true. Added a pre-reboot hook script to verify NVIDIA driver compatibility before allowing node reboot. Pinned kernel version on GPU nodes as additional safeguard using apt-mark hold.","services":["gpu-worker-nodes","nvidia-driver","cuda-runtime","training-cluster","kured"],"service":"gpu-worker-nodes","severity":"P1","timestamp":"2026-01-25T07:50:00Z","tags":["nvidia","cuda","driver","kernel","dkms","gpu","linux","kured"]}