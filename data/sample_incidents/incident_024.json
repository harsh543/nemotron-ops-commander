{"id":"INC-024","title":"Python asyncio RuntimeError in FastAPI with torch.cuda","summary":"FastAPI inference service crashed on startup with 'RuntimeError: Cannot run the event loop while another is running' when attempting to initialize a PyTorch CUDA model inside an async endpoint. The service used uvicorn with a lifespan handler that called torch.cuda.is_available() and loaded a model checkpoint during the async startup event. The error occurred because torch.cuda internal calls created a nested event loop conflicting with uvicorn's running asyncio loop. Affected 3 replicas of the ml-inference deployment, causing complete inference outage for 25 minutes.","source":"stackoverflow","root_cause":"PyTorch's CUDA initialization internally uses synchronous blocking calls that are incompatible with a running asyncio event loop. Calling torch.cuda.is_available() and torch.load() inside an async context (FastAPI lifespan async generator) triggered a nested event loop conflict. The issue was exacerbated by the nest_asyncio package not being installed and the CUDA lazy init environment variable CUDA_MODULE_LOADING not being set to LAZY.","resolution":"Moved model loading to a synchronous function executed via asyncio.get_event_loop().run_in_executor(None, load_model) within the lifespan handler. Set CUDA_MODULE_LOADING=LAZY environment variable in the deployment manifest. Added nest_asyncio as a fallback. Wrapped torch.cuda initialization in a separate thread using concurrent.futures.ThreadPoolExecutor. Service startup time improved from timeout to 8 seconds.","services":["ml-inference","fastapi-server","model-registry"],"service":"ml-inference","severity":"P2","timestamp":"2026-01-18T14:30:00Z","tags":["python","asyncio","fastapi","pytorch","cuda","event-loop","runtime-error"]}